"use strict";
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT License.
Object.defineProperty(exports, "__esModule", { value: true });
exports.LogsIngestionClient = void 0;
const index_js_1 = require("./api/index.js");
const operations_js_1 = require("./api/operations.js");
const models_js_1 = require("./models/models.js");
const core_util_1 = require("@azure/core-util");
const gZippingPolicy_js_1 = require("./gZippingPolicy.js");
const concurrentPoolHelper_js_1 = require("./utils/concurrentPoolHelper.js");
const splitDataToChunksHelper_js_1 = require("./utils/splitDataToChunksHelper.js");
const DEFAULT_MAX_CONCURRENCY = 5;
/**
 * Client for Monitor Logs Ingestion
 */
class LogsIngestionClient {
    /**
     * Construct a MonitorIngestionClient that can be used to query logs using the Log Analytics Query language.
     *
     * @param tokenCredential - A token credential.
     * @param options - Options for the MonitorIngestionClient.
     */
    constructor(endpoint, tokenCredential, options) {
        var _a;
        const prefixFromOptions = (_a = options === null || options === void 0 ? void 0 : options.userAgentOptions) === null || _a === void 0 ? void 0 : _a.userAgentPrefix;
        const userAgentPrefix = prefixFromOptions
            ? `${prefixFromOptions} azsdk-js-client`
            : `azsdk-js-client`;
        this._client = (0, index_js_1.createLogsIngestion)(endpoint, tokenCredential, Object.assign(Object.assign({}, options), { userAgentOptions: { userAgentPrefix } }));
        this.pipeline = this._client.pipeline;
        this.pipeline.addPolicy(gZippingPolicy_js_1.GZippingPolicy);
    }
    /**
     * Uploads logs to Monitor Resource
     * @param ruleId - The immutable Id of the Data Collection Rule resource.
     * @param streamName - The streamDeclaration name as defined in the Data Collection Rule.
     * @param logs - An array of objects matching the schema defined by the provided stream.
     * @param options - The options parameters.
     * See error response code and error response message for more detail.
     */
    async upload(ruleId, streamName, logs, options) {
        // TODO: Do we need to worry about memory issues when loading data for 100GB ?? JS max allocation is 1 or 2GB
        var _a;
        // This splits logs into 1MB chunks
        const chunkArray = (0, splitDataToChunksHelper_js_1.splitDataToChunks)(logs);
        const concurrency = Math.max((_a = options === null || options === void 0 ? void 0 : options.maxConcurrency) !== null && _a !== void 0 ? _a : DEFAULT_MAX_CONCURRENCY, 1);
        const uploadResultErrors = [];
        await (0, concurrentPoolHelper_js_1.concurrentRun)(concurrency, chunkArray, async (eachChunk) => {
            try {
                await (0, operations_js_1.upload)(this._client, ruleId, streamName, eachChunk, {
                    contentEncoding: "gzip",
                    abortSignal: options === null || options === void 0 ? void 0 : options.abortSignal,
                });
            }
            catch (e) {
                if (options === null || options === void 0 ? void 0 : options.onError) {
                    options.onError({
                        failedLogs: eachChunk,
                        cause: (0, core_util_1.isError)(e) ? e : new Error(e),
                    });
                }
                uploadResultErrors.push({
                    cause: (0, core_util_1.isError)(e) ? e : new Error(e),
                    failedLogs: eachChunk,
                });
            }
        }, options === null || options === void 0 ? void 0 : options.abortSignal);
        if (uploadResultErrors.length > 0) {
            throw new models_js_1.AggregateLogsUploadError(uploadResultErrors);
        }
        return Promise.resolve();
    }
}
exports.LogsIngestionClient = LogsIngestionClient;
//# sourceMappingURL=logsIngestionClient.js.map
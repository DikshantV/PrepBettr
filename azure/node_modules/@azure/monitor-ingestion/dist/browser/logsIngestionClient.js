// Copyright (c) Microsoft Corporation.
// Licensed under the MIT License.
import { createLogsIngestion } from "./api/index.js";
import { upload } from "./api/operations.js";
import { AggregateLogsUploadError } from "./models/models.js";
import { isError } from "@azure/core-util";
import { GZippingPolicy } from "./gZippingPolicy.js";
import { concurrentRun } from "./utils/concurrentPoolHelper.js";
import { splitDataToChunks } from "./utils/splitDataToChunksHelper.js";
const DEFAULT_MAX_CONCURRENCY = 5;
/**
 * Client for Monitor Logs Ingestion
 */
export class LogsIngestionClient {
    /**
     * Construct a MonitorIngestionClient that can be used to query logs using the Log Analytics Query language.
     *
     * @param tokenCredential - A token credential.
     * @param options - Options for the MonitorIngestionClient.
     */
    constructor(endpoint, tokenCredential, options) {
        var _a;
        const prefixFromOptions = (_a = options === null || options === void 0 ? void 0 : options.userAgentOptions) === null || _a === void 0 ? void 0 : _a.userAgentPrefix;
        const userAgentPrefix = prefixFromOptions
            ? `${prefixFromOptions} azsdk-js-client`
            : `azsdk-js-client`;
        this._client = createLogsIngestion(endpoint, tokenCredential, Object.assign(Object.assign({}, options), { userAgentOptions: { userAgentPrefix } }));
        this.pipeline = this._client.pipeline;
        this.pipeline.addPolicy(GZippingPolicy);
    }
    /**
     * Uploads logs to Monitor Resource
     * @param ruleId - The immutable Id of the Data Collection Rule resource.
     * @param streamName - The streamDeclaration name as defined in the Data Collection Rule.
     * @param logs - An array of objects matching the schema defined by the provided stream.
     * @param options - The options parameters.
     * See error response code and error response message for more detail.
     */
    async upload(ruleId, streamName, logs, options) {
        // TODO: Do we need to worry about memory issues when loading data for 100GB ?? JS max allocation is 1 or 2GB
        var _a;
        // This splits logs into 1MB chunks
        const chunkArray = splitDataToChunks(logs);
        const concurrency = Math.max((_a = options === null || options === void 0 ? void 0 : options.maxConcurrency) !== null && _a !== void 0 ? _a : DEFAULT_MAX_CONCURRENCY, 1);
        const uploadResultErrors = [];
        await concurrentRun(concurrency, chunkArray, async (eachChunk) => {
            try {
                await upload(this._client, ruleId, streamName, eachChunk, {
                    contentEncoding: "gzip",
                    abortSignal: options === null || options === void 0 ? void 0 : options.abortSignal,
                });
            }
            catch (e) {
                if (options === null || options === void 0 ? void 0 : options.onError) {
                    options.onError({
                        failedLogs: eachChunk,
                        cause: isError(e) ? e : new Error(e),
                    });
                }
                uploadResultErrors.push({
                    cause: isError(e) ? e : new Error(e),
                    failedLogs: eachChunk,
                });
            }
        }, options === null || options === void 0 ? void 0 : options.abortSignal);
        if (uploadResultErrors.length > 0) {
            throw new AggregateLogsUploadError(uploadResultErrors);
        }
        return Promise.resolve();
    }
}
//# sourceMappingURL=logsIngestionClient.js.map